# 智能上下文管理功能

## 功能概述

当对话历史接近模型的token限制时，系统会自动总结历史对话内容，为新的对话腾出空间，避免简单粗暴的截断。

## 工作原理

### 1. 自动检测
- 当上下文使用率超过75%时触发智能管理
- 系统会保留最近的5条消息
- 将更早的消息发送给LLM进行总结

### 2. 智能总结
- 使用当前配置的LLM模型生成总结
- 总结包含：
  - 主要讨论的话题和问题
  - 重要的决定和结论
  - 正在进行的任务或项目状态
  - 需要记住的重要上下文信息

### 3. 上下文重构
- 用总结内容替换旧消息
- 保留最近的对话内容
- 确保对话连续性

## 用户体验

### 视觉提示
- 显示"📝 智能上下文管理"卡片
- 说明总结了多少条历史消息
- 可展开查看详细总结内容

### 降级处理
- 如果总结失败，自动降级到简单截断
- 提取关键词作为备用总结方案
- 确保系统稳定运行

## 技术实现

### 核心组件
- `ContextManager.needsContextSummarization()` - 检测是否需要总结
- `ContextManager.getMessagesForSummarization()` - 获取需要总结的消息
- `ContextManager.applySummarization()` - 应用总结结果
- `AgentEngine.summarizeContext()` - 生成智能总结

### 事件类型
```typescript
{ 
  type: 'context_summarized'; 
  summary: string; 
  summarizedCount: number 
}
```

## 配置参数

- **检测阈值**: 75% token使用率
- **保留消息数**: 最近5条消息
- **总结长度**: 最多1000 tokens
- **温度参数**: 0.3 (确保总结准确性)

## 优势

1. **智能化**: 保留重要信息而非简单截断
2. **连续性**: 维持对话上下文的完整性
3. **透明性**: 用户可以查看总结内容
4. **稳定性**: 有降级方案确保系统可用
5. **自动化**: 无需用户手动干预

这个功能让长对话变得更加智能和用户友好，避免了传统截断方式的信息丢失问题。