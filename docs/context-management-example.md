# 智能上下文管理使用示例

## 场景演示

假设你正在与AI进行一个长时间的编程讨论：

### 1. 初始对话
```
用户: 我想创建一个React组件来显示用户列表
AI: 我来帮你创建一个用户列表组件...

用户: 能不能添加搜索功能？
AI: 当然可以，我来添加搜索功能...

用户: 还需要分页功能
AI: 好的，我来实现分页...

[继续对话30多轮...]
```

### 2. 接近Token限制
当对话历史接近模型token限制时（通常是75%），系统会自动触发智能管理：

```
🤖 对话历史较长，正在智能总结上下文...

📝 智能上下文管理
已总结 25 条历史消息，为新对话腾出空间

[查看总结内容 ▼]
总结：用户正在开发一个React用户列表组件，已实现基本列表显示、搜索功能和分页功能。
讨论了组件的状态管理、API调用优化、错误处理等技术细节。当前正在优化性能，
考虑使用虚拟滚动来处理大量数据。用户偏好使用TypeScript和函数式组件。
```

### 3. 继续对话
总结完成后，对话可以继续进行，系统保留了：
- 上下文总结（包含重要信息）
- 最近5条对话（保持连续性）

```
用户: 现在我想添加导出功能
AI: 基于我们之前讨论的用户列表组件，我来帮你添加导出功能...
```

## 技术细节

### 触发条件
- Token使用率 > 75%
- 消息数量 > 5条

### 保留策略
- 保留最近5条消息
- 其余消息发送给LLM总结
- 总结作为系统消息插入

### 降级处理
如果LLM总结失败：
```
🤖 上下文总结失败，已自动清理 15 条旧消息
```

## 用户体验

### 优势
1. **无感知**: 自动处理，无需用户干预
2. **智能化**: 保留重要信息而非简单截断
3. **透明性**: 可以查看总结内容
4. **连续性**: 对话流程不中断

### 与传统截断的对比

**传统方式**:
```
❌ 对话历史过长，已自动清理 20 条旧消息
```
- 信息完全丢失
- 上下文断裂
- 用户体验差

**智能管理**:
```
✅ 📝 智能上下文管理
已总结 20 条历史消息，为新对话腾出空间
[可查看详细总结]
```
- 重要信息保留
- 上下文连续
- 用户体验佳

## 配置选项

开发者可以调整以下参数：

```typescript
// 检测阈值（默认75%）
contextManager.needsContextSummarization(0.8);

// 保留消息数（默认5条）
contextManager.getMessagesForSummarization(3);

// 总结参数
{
  maxTokens: 1000,    // 总结长度
  temperature: 0.3    // 确保准确性
}
```

这个功能让长对话变得更加智能和用户友好！